<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Study on Efficient Models 2 | Yuhang Song - BLOG | 宋宇航</title>
    <meta name="author" content="Yuhang  Song">
    <meta name="description" content="Walkthrough more SOTA compact models.">
    <meta name="keywords" content="Yuhang Song, university of liverpool, national tsing hua university, 宋宇航, 英国利物浦大学, 国立清华大学, topsoft, topsoftint, topsoft 极致网络">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    
    <!-- Sidebar Table of Contents -->
    <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet">
    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;YH&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://huskykingdom.github.io/blog/2022/em2/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Yuhang Song - BLOG | 宋宇航</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">Curriculum Vitae</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        
        <div class="row">
          <!-- sidebar, which will move to the top on a small screen -->
          <div class="col-sm-3">
            <nav id="toc-sidebar" class="sticky-top"></nav>
          </div>
          <!-- main content area -->
          <div class="col-sm-9">
            <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Study on Efficient Models 2</h1>
    <p class="post-meta">October 28, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/images">
          <i class="fas fa-hashtag fa-sm"></i> images,</a>  
          <a href="/blog/tag/links">
          <i class="fas fa-hashtag fa-sm"></i> links,</a>  
          <a href="/blog/tag/math">
          <i class="fas fa-hashtag fa-sm"></i> math</a>  
          
        ·  
        <a href="/blog/category/studynote">
          <i class="fas fa-tag fa-sm"></i> studynote</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>Download full version of this notes with more details and images <a href="/assets/pdf/Notes.pdf">here</a>.</p>

<h2 id="efficientnet-v1-2019">EfficientNet V1 (2019)</h2>
<h3 id="idea--background">Idea &amp; Background</h3>
<p>The paper[0] proposed that, modern CNNs were developed with more layers(deeper), more channels(wider), and higher quality of input images(hgher resolutions). However, scaling up any of the parameters mentioned monotonically to a large number would not very much benefits the model, in particular, the model might in this case, reaches an accuracy saturation.</p>

<p>The paper evaluated CNN models that monotonically scalling up its width $w$, depth $d$ and resolution $r$, the improvements is significant until the scaling reaches a certain limit.</p>

<p>The paper concludes that:</p>

<ul>
  <li>
<strong>Observation 1</strong> – Scaling up any dimension of network
width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.</li>
</ul>

<p>Intuitively, increased input resolutions needs wider networks that are able to capture more fine-grained patterns with more pixels, as well as the higher depth such that the larger receptive fields would help capture similar features that include more pixels.</p>

<p>These result lead us to the second observation:</p>

<ul>
  <li>
<strong>Observation 2</strong> – In order to pursue better accuracy and
efficiency, it is critical to balance all dimensions of network
width, depth, and resolution during ConvNet scaling.</li>
</ul>

<p>Since the current existing methods are adjusting width, depth, resolutions manually, the paper hence propose an new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective <strong>compound coefficient</strong>.</p>

<h3 id="compound-coefficient-scaling">Compound Coefficient Scaling</h3>

<p>Define a compound coefficient \(\phi\) that is used to uniformly scales network width, depth, and resolution in a principled way:</p>

<p>depth: \(d = \alpha^\phi\)</p>

<p>width: \(w = \beta^\phi\)</p>

<p>resolution: \(r = \gamma^\phi\)</p>

<p>s.t. \(\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\)</p>

\[\alpha \ge 1, \beta \ge 1, \gamma \ge 1\]

<p>From which, the \(\alpha,\beta,\gamma\) are constants that can be determined by a small grid search, \(\phi\) is a user-defined coefficient that controls how many more resources are available for model scalling.</p>

<p>The paper point out that, the FLOPSof a regular convolution operation is proportional to \(d,w^2,r^2\), that is in other words, 2X network depth will gain 2X FLOPS, but 2X network width or resolution will increase FLOPS by 4X. Also, because convolutional layers are usually dominate the computation cost in ConvNets, scalling a ConvNet with above equation will approximately increase total FLOPS by \((\alpha \cdot \beta^2 \cdot \gamma^2)^\phi\), the method constraints \(\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\) so that the total FLOPS will approximately increase by \(2^\phi\).</p>

<h3 id="efficientnet-architecture">EfficientNet Architecture</h3>

<p>The EfficientNet is based on MnasNet (by Platform-aware Neural Architecture Search), except EfficientNet-B0 is slightly bigger with FLOPS targets set to 400M.</p>

<p>In which, the MBConv block is mobile inverted bottleneck, the SE block is added into each block for optimization.</p>

<p>Starting with EfficientNet-B0, the compound scaling method is performed with 2 steps:</p>

<ul>
  <li>STEP 1: first fix φ = 1, assuming twice more resources available, and do a small grid search of \(\alpha,\beta,\gamma\)
based on equation shown above. In particular, the paper found
the best values for EfficientNet-B0 are \(α = 1.2\), \(β =
1.1\), \(γ = 1.15\), under constraint of \(α · β^2 · γ^2 ≈ 2\).</li>
  <li>STEP 2: fix \(\alpha,\beta,\gamma\) as constants and scale up baseline network with different \(\phi\) using above equation, to obtain EfficientNet-B1 to B7.</li>
</ul>

<h2 id="efficientnet-v2-2021">EfficientNet V2 (2021)</h2>

<h3 id="background--idea">Background &amp; Idea</h3>
<p>EfficientNet V2 has got improved training speed and better performance than EfficientNet V1. In this upgraded version, we focus not only on the accuracy and #parameters/FLOPs, but jointly focusing on the training efficiency as well.</p>

<p>The paper[1] identifies several problems of the previouse EfficientNet V1:</p>

<ul>
  <li>Training with very large image sizes is slow. <strong>(Proposed Solution: Progressive Learning)</strong>
</li>
  <li>Depthwise Convolutions are slow in early layers but effective in later layers, since the depthwise convolutions often cannot fully utilize modern accelerators. <strong>(Proposed Solution: Replacing MBConv layers by  Fused-MBConv via NAS)</strong>
</li>
  <li>Equally scaling up every stage is sub-optimal. <strong>(Proposed Solution: New Scaling Rule and Restriction)</strong>
</li>
</ul>

<p>The Fused-MBConv (better utilize mobile or server accelerators) is replacing the original expension layer and depthwise convolution layer of MBConv by a single 3x3 convolution, the paper evaluated that by using Fused-MBConv in early stage(1-3) helps accelerate the training step with a small overhead on parameters and FLOPs. NAS is used to automatically search for the best combination.</p>

<h3 id="training-aware-nas-and-scaling">Training-Aware NAS and Scaling</h3>
<p>The Training-Aware NAS is based on Platform-Aware NAS[2], which its search space is also a stage-based factorized space, with the following search options:</p>

<p>Convolution Ops : {MBConv, Fused-MBConv}</p>

<p>Kernel Size: {3x3, 5x5}</p>

<p>Expension Ratio: {1,4,6}</p>

<p>However, in Training-Aware NAS, the paper point out that, they removed unnecessary search options like skip ops, and resued the same channel sizes from the backbone as they aree already searched. In addition, the search reward in Training-Aware NAS conbines model accuracy \(A\), the normalized traning step time \(S\), and the parameter size \(P\), by using a simple weighted product \(A \cdot S^w \cdot P^v\), where \(w=-.0.07\) and \(v=-0.05\),  empirically determined to balance the trade-offs similar to [2].</p>

<p>As for the scaling part, EfficientNetV2-S is scaled up to EfficientNetV2-M/L using compound scaling with optimizations: (1) Restrict maximum inference image size to 480. (2) Gradually add more layers to later stages to increase the network capacity without adding much runtime overhead.</p>

<h3 id="progressive-learning">Progressive Learning</h3>

<p>The main idea of progressive learning is to increase image size and regularzation magnitude at the same time during the training stage. The paper agure that the loss of accuracy of only progressively enlarge input image size during training drops due to unbalanced regularization.</p>

<h2 id="references">References</h2>

<p>[0] Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR</p>

<p>[1] Tan, M., &amp; Le, Q. (2021, July). Efficientnetv2: Smaller models and faster training. In International Conference on Machine Learning (pp. 10096-10106). PMLR.</p>

<p>[2] Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., &amp; Le, Q. V. (2019). Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2820-2828).</p>

<p>[3] Brock, A., De, S., Smith, S. L., &amp; Simonyan, K. (2021, July). High-performance large-scale image recognition without normalization. In International Conference on Machine Learning (pp. 1059-1071). PMLR.</p>

    </div>
  </article><div id="disqus_thread" style="max-width: 800px; margin: 0 auto;">
    <script type="text/javascript">
        var disqus_shortname  = 'al-folio';
        var disqus_identifier = '/blog/2022/em2';
        var disqus_title      = "Study on Efficient Models 2";
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a>
</noscript>
</div>
</div>

          </div>
        </div>
        
      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Yuhang  Song. Powered by <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>
  <!-- Sidebar Table of Contents -->
  <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>


  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
