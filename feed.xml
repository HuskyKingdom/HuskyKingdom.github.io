<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://huskykingdom.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://huskykingdom.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-02T12:25:15+00:00</updated><id>https://huskykingdom.github.io/feed.xml</id><title type="html">Yuhang Song | BLOG</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Installation Updates of Sim-to-Real-Virtual-Guidance-for-Robot-Navigation</title><link href="https://huskykingdom.github.io/blog/2023/frame_install/" rel="alternate" type="text/html" title="Installation Updates of Sim-to-Real-Virtual-Guidance-for-Robot-Navigation" /><published>2023-07-02T04:00:00+00:00</published><updated>2023-07-02T04:00:00+00:00</updated><id>https://huskykingdom.github.io/blog/2023/frame_install</id><content type="html" xml:base="https://huskykingdom.github.io/blog/2023/frame_install/"><![CDATA[<p>The article provides clear instructions on how to install necessary environments for all modules in <a href="https://github.com/KaiChen1008/Sim-to-Real-Virtual-Guidance-for-Robot-Navigation">Sim-To-Real Navigation Robots</a> project, apart from the official <a href="https://kaichen1008.github.io/Sim-to-Real-Virtual-Guidance-for-Robot-Navigation/">original documentation</a> provided, since it was made in 2020, some additional corrections are needed during the installation.</p>

<p>Moreover, there are some missing files in the original repo, they were also provided here. Note that this article will only go though the updates to the original documentation, please refer to the original documentation for full instructions.</p>

<h2 id="orb-slam2-installation">ORB-SLAM2 Installation</h2>

<hr />

<h3 id="opencv-32">OpenCV 3.2</h3>

<p><strong>1. Missing stdlib.h</strong></p>

<p>In section 3.3, change the original build &amp; install command into the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cmake -DCMAKE_BUILD_TYPE=Release \
      -DWITH_CUDA=OFF \
      -DCMAKE_INSTALL_PREFIX=/usr/local/opencv-${CV3_RELEASE} \
      -DOPENCV_EXTRA_MODULES_PATH=/tmp/opencv_contrib/modules \
      /tmp/opencv
      -DENABLE_PRECOMPILED_HEADERS=OFF
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">-DENABLE_PRECOMPILED_HEADERS=OFF</code> is added, without this line of parameter might result in error of missing <em>stdlib.h</em> (fatal error: stdlib.h: No such file or directory #include_next stdlib.h).</p>

<p><strong>2. Specify Number of Parallel Building Threads</strong></p>

<p>If you are running Ubuntu 18.04 in virtual machine, it is likely that <code class="language-plaintext highlighter-rouge">proc</code> variable does not work, hence leading to an building error.</p>

<p>Make OpenCV by explicitly specify the number of threads would address this problem, this is also recommended:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo make install -j8
</code></pre></div></div>

<p>In this case 8 threads will be used to build the project.</p>

<h3 id="pangolin">Pangolin</h3>

<p>Do not just clone the Pangolin repo directly, Pangolin has several upgrades to the new version, which were made compilable with C++ 17. However, C++ 17 is not installed in Ubuntu 18.04 by default, install latest version of Pangolin in lower version C++ will cause error [Pangolin could not be found because dependency Eigen3 could not be found.].</p>

<p>Download Pangolin v0.5 <a href="https://github.com/stevenlovegrove/Pangolin/tags">here</a> and build &amp; install it as detailed in original instruction.</p>

<p>Make sure you specify threads while building Pangolin if needed.</p>

<h3 id="install-orb-slam2">Install ORB-SLAM2</h3>

<p>Clone the repo to your local environment first as instructed by the original documentation. Copy <em>modified_ORB_SLAM2</em> folder in <em>localization_module</em> folder to the <code class="language-plaintext highlighter-rouge">src</code> directory of your ROS workspace, and rename it to <em>ORB_SLAM2</em>.</p>

<p>In additional, you must do the following first <strong>before</strong> build ORB-SLAM2.</p>

<p><strong>1. Missing tar file</strong></p>

<p>In <em>built.bash</em>, file <em>ORBvoc.txt.tar.gz</em> is referenced but not provided. Download the file <a href="https://github.com/raulmur/ORB_SLAM2/blob/master/Vocabulary/ORBvoc.txt.tar.gz">here</a> and put it into <em>ORB-SLAM2/Vocabulary/</em>.</p>

<p><strong>2. Missing Examples Folder</strong></p>

<p>The original <strong>Examples</strong> folder provided is not complete. Replace the whole folder with the one appears in <a href="https://github.com/raulmur/ORB_SLAM2/tree/master/Examples">this repo.</a></p>

<p><strong>3. Update CMakeList.txt</strong></p>

<p>Update <em>CMakeList.txt</em> in <em>ORB_SLAM2_Examples/ROS/ORB_SLAM2/</em> by adding a boost library line <code class="language-plaintext highlighter-rouge">-lboost_syste</code> into the library section:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set(LIBS
${OpenCV_LIBS}
${EIGEN3_LIBS}
${Pangolin_LIBRARIES}
${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so
${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so
${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so
-lboost_system
)

</code></pre></div></div>

<p>Now you can build &amp; install ORB-SLAM2 and ORB-SLAM2 node as instructed by the original documentation.</p>]]></content><author><name></name></author><category term="tutorial" /><summary type="html"><![CDATA[Additional installation steps for old repo.]]></summary></entry><entry><title type="html">Advanced Reinforcement Learning</title><link href="https://huskykingdom.github.io/blog/2022/rl2/" rel="alternate" type="text/html" title="Advanced Reinforcement Learning" /><published>2022-11-08T15:59:00+00:00</published><updated>2022-11-08T15:59:00+00:00</updated><id>https://huskykingdom.github.io/blog/2022/rl2</id><content type="html" xml:base="https://huskykingdom.github.io/blog/2022/rl2/"><![CDATA[<h2 id="advanced-reinforcement-learning">Advanced Reinforcement Learning</h2>
<h3 id="11-actor-critic-reinforcement-learning">1.1 Actor-Critic Reinforcement Learning</h3>

<p>Reinforcement Learning learns a policy from environmental experiences, it gains reward signals from environment and accordingly adjust the model to maximize the expected rewards and thus formulate a policy. Value-based Reinforcement Learning define value functions that evaluate the states and actions of a markov decision process, the optimal policy is typically selected from greedy policy. Whereas the Policy-based Reinforcement Learning algorithms model the policy explicitly and optimize the policy by methods like gradient ascent.</p>

<p>Another group of algorithm integrates the advantages of both Value-based methods and Policy-based methods, namely Actor-Critic architectures, in Actor-Critic, we define a critic:</p>

\[Q_w(s,a) \approx Q^{\pi_\theta}(s,a)\]

<p>Where the critic approximates state-action value function \(Q(s,a)\), the actor approximates the policy \(\pi\), there are parameterized by \(w\) and \(\theta\) respectively.</p>

<p>Actor-critic algorithm follow an approximate policy gradient, the actor network can be updated by:</p>

\[\nabla_\theta J(\theta) \approx \mathbb{E}_{\pi_\theta}[\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)] \\

\nabla \theta = \alpha \nabla_\theta log \pi_\theta(s,a) Q_w(s,a) \\

\theta_{t+1} \leftarrow \theta_t + \lambda \nabla\theta_t\]

<p>The critic network is based on critic functions, here use Q-function as an example, the critic network can be updated as:</p>

\[\nabla_w J(w) \approx \mathbb{E}_{\pi_\theta}[MSE(Q_{t},R_{t+1}+max_a Q_{t+1})] \\

\nabla w = MSE(Q_{t},R_{t+1}+max_a Q_{t+1}) \\

w_{t+1} \leftarrow w_t + \lambda \nabla w_t\]

<p>Instead of leting critic to estimate state-value function, we can allow it to alternatively estimates Advantage function \(A(s,a) = Q_w(s,a) - V_v(s)\) to reduce the variance. There are many alternative critic function choices.</p>

<h3 id="12-proximal-policy-optimization-with-ac-based-advantage-function">1.2 Proximal Policy Optimization with AC-based Advantage Function</h3>

<p>Baseline algorithm of OpenAI. PPO allows off-policy learning to policy gradient algorithm. In policy gradient, we update our policy network by compute gradient of the expected reward function with respect to policy parameters \(\theta\), and perform gradient acsent to maximize it:</p>

\[\nabla J(\theta) = \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n) \\
= \mathbb{E}_{\tau \backsim \pi_\theta}[R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n)] \\
\theta \leftarrow \theta + \alpha \nabla \bar{R}_\theta\]

<p>In PPO algorithm, instead of sampling trajectories from policy \(\pi_\theta\), in order to increase sample efficiency(reuse expirence), we sample trajectories from another policy \(\pi_{\theta'}\) and apply a importance sampling method to correct the difference.</p>

\[J_{\theta'}(\theta) =  \mathbb{E}_{\tau \backsim \pi_{\theta'}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta'}(a_t|s_t)} R(\tau^n) ]  \\
\nabla J_{\theta'}(\theta) =  \mathbb{E}_{\tau \backsim \pi_{\theta'}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta'}(a_t|s_t)} R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n)]\]

<p>The reward \(R(\tau^n)\) can be replaced by advantage function \(A^{\theta'} (s_t,a_t)\), where we levarage the power of AC architecture the gain more suitable representation of the loss to optimize.</p>

\[J_{\theta'}(\theta) =  \mathbb{E}_{\tau \backsim \pi_{\theta'}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta'}(a_t|s_t)} A^{\theta'} (s_t,a_t)]  \\


\nabla J_{\theta'}(\theta) =  \mathbb{E}_{\tau \backsim \pi_{\theta'}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta'}(a_t|s_t)} A^{\theta'} (s_t,a_t) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n)]\]

<h3 id="121-ppo-kl-regularzation">1.2.1 PPO KL Regularzation</h3>

<p>In addition, we need to add a regularzation term (or in TRPO, add a constrain) to the objective function to constrain the difference between two distributions, therefore the objective function becomes:
\(J(\theta) = J_{\theta'}(\theta) - \beta KL(\theta,\theta')\)
Where adaptively set the value of \(\beta\), specificlly when \(KL(\theta,\theta') &gt; KL_{max}\), increase \(\beta\); when \(KL(\theta,\theta') &lt; KL_{min}\), decrease \(\beta\).</p>

<h3 id="121-ppo-clip-method">1.2.1 PPO Clip Method</h3>

\[J_{clip}(\theta) \approx \sum_{s_t,a_t} min(\frac{p_\theta(a_t | s_t)}{p_{\theta'}(a_t|s_t)} A^{\theta'}(s_t,a_t), \ clip(\frac{p_\theta(a_t | s_t)}{p_{\theta'}(a_t|s_t)}, 1 - \epsilon , 1 + \epsilon) \ A^{\theta'}(s_t,a_t))\]

<p>Where \(clip\) =  \(1-\epsilon\) if \(\frac{p_\theta(a_t \ given \ s_t)}{p_{\theta'}(a_t \ given \ s_t)} &lt; 1 - \epsilon\) ; \(clip\) =  \(1+\epsilon\) if \(\frac{p_\theta(a_t \ given \ s_t)}{p_{\theta'}(a_t \ given \ s_t)} &gt; 1 + \epsilon\) ; values that fall anywhere in between \(clip = \frac{p_\theta(a_t \ given \ s_t)}{p_{\theta'}(a_t \ given \ s_t)}\). This methods hence set an constrain that making sure the differences between two policy distributions changes within certain range of \(\epsilon\).</p>]]></content><author><name></name></author><category term="studynote" /><category term="images," /><category term="math" /><summary type="html"><![CDATA[PPO, AC]]></summary></entry><entry><title type="html">Reinforcement Learning Basics</title><link href="https://huskykingdom.github.io/blog/2022/rl1/" rel="alternate" type="text/html" title="Reinforcement Learning Basics" /><published>2022-10-30T15:59:00+00:00</published><updated>2022-10-30T15:59:00+00:00</updated><id>https://huskykingdom.github.io/blog/2022/rl1</id><content type="html" xml:base="https://huskykingdom.github.io/blog/2022/rl1/"><![CDATA[<p>Download full version of this notes with more details and images <a href="/assets/pdf/Notes.pdf">here</a>.</p>

<h1 id="reinforcement-learning">Reinforcement Learning</h1>

<h2 id="model-based-rl-dynamic-programming"><strong>Model-based RL: Dynamic Programming</strong></h2>

<p>In MDP, we want to model the state value function and state-action value functions, besed on which, we can form a strategy that greedly select actions with max state-action value in for each individual state.</p>

<p>We define state value function \(v_\pi\) and stage-action value function \(q_\pi\) for a certain policy \(\pi\) as following:</p>

\[q_\pi(s,a) = \mathbb{E}_{\tau \backsim \pi}[R(\tau) | S_0=s, A_0=a] \\
v_\pi(s) = \mathbb{E}_{a \backsim \tau}[q_\pi(s,a)]\]

<p>The Bellman Expectation Functions provids us a way to iteratively calculate value functions by decompose them into immediate reward plus discounted value of successor state.</p>

\[v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \\

q_\pi(s,a) = \mathbb{E}_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1} | S_t=s,A_t=a)]\]

<p>The Bellman Equations can be solved directly if we have full information of the environment(i.e. we know the state transformation function), in discrete finite state environment:
\(v = r + \gamma Pv\)</p>

<p>From which \(v\) and \(r\) are scalars, \(P\) is state transform probability matrix. Solve it directly we get:</p>

\[v = (I-\gamma P)^{-1}r\]

<p>The Bellman Optimally Equation can be then written as:</p>

\[v_*(s) = max_{a}\mathbb{E}[R_s^a + \gamma  v_*(s')] = max_{a} R_s^a + \gamma \sum_{s' \in S} p_{ss'}^a v_*(s') \\ 

q_*(s,a) =  R_s^a + \gamma \mathbb{E}[max_{a'} q_*(s',a')] = R_s^a + \gamma \sum_{s' \in S} p_{ss'}^a max_{a'} q_*(s',a')\]

<p>The complexity of solving Bellman Expectation equation is \(O(n^3)\), where \(n\) is the number of states, that means it is hard to solve when having large state space. In such case, we need to use methods like Dynamic Programming, Monte-Carlo Estimation, or Temporal Difference. In other hand, Bellman Optimalityt Equations are non-linear, and has no closed form solution(in general), therefore cannot be directly solved, we need to use other methods.</p>

<p>Dynamic Programming iteratively solves large scale questions by decomposite them into smaller ones, those questions have to be:</p>

<ul>
  <li>With Optimal Substructure</li>
  <li>Overlapping Subproblems</li>
</ul>

<p>MDP satisfy both propeerties. We can therefore use DP to solve MDP questions, <strong>note that DP solutions requires Full Knowledge of the MDP, and are hence Model-Based RL methods.</strong></p>

<h3 id="policy-iteration">Policy Iteration</h3>

<p>Policy Iteration method evaluate a given policy \(\pi\) by dynamic programming, it iteratively use Bellman Expectations to evaluate the state function of given policy \(\pi\). Specifically for each iteration \(k\):</p>

\[v_{k+1}(s) = \sum_{a \in A} \pi(a|s) (R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_k(s'))\]

<p>To improve the policy, acting greedily with respect to \(v_\pi\): 
\(\pi' = greedy(v_\pi) = argmax_{a \in A} \ q_\pi(s,a)\)</p>

<p>The algorithm converges to \(v_*(s)\) with greedy policy imrovement, otherwise converges to real \(v_\pi(s)\).</p>

<h3 id="value-iteration">Value Iteration</h3>

<p>Based on Principle of Optimality, which states a policy \(\pi(s)\) is an optimal policy on state \(s\) if and only if \(\pi(s)\) achives \(v_\pi(s') = v_*(s')\) for any state \(s'\) that is reachable from \(s\). From which, it implies if we know the solution of \(v_*(s')\), we can figure out the optimal solution to any state \(s\) by <strong>One-Step Full Backup</strong>.</p>

<p>Formally, if we know the solution to subproblems \(v_*(s')\), the solution \(v_*(s)\) can be found be one-step lookahead:</p>

\[v_*(s) = max_{a \in A} R_s^a + \gamma \sum_{s' \in S} p_{ss'}^a v_*(s')\]

<p>The algorithm converges to \(v_*(s)\).</p>

<p>In summery:</p>

<table>
  <thead>
    <tr>
      <th>Problem</th>
      <th>Bellman Equation</th>
      <th>Method (Algorithm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Value Function Prediction</td>
      <td>Bellman Expectation Equation</td>
      <td>Policy Iteration</td>
    </tr>
    <tr>
      <td>Control</td>
      <td>Bellman Expectation Equation</td>
      <td>Policy Iteration + Greedy Policy Improvement</td>
    </tr>
    <tr>
      <td>Control</td>
      <td>Bellman Optimality Equation</td>
      <td>Value Iteration</td>
    </tr>
  </tbody>
</table>

<h3 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h3>

<p>DP methods described above used synchronous backups, where all states are backed up in parallel. Asynchronous DP backs up states individually, in any order, can significantly reduce computation. It is guaranteed to converge if all states continue to be selected.</p>

<p>Three simple ideas for asynchronous dynamic programming:</p>
<ul>
  <li>In-place dynamic programming</li>
  <li>Prioritised sweeping</li>
  <li>Real-time dynamic programming</li>
</ul>

<p><br /></p>

<h2 id="model-free-value-based-methods"><strong>Model-free Value-based Methods</strong></h2>

<p>Dynamic Programming RL methods are all model based methods, in which we need specific environment model to excute them, it is common in real-world RL environment that we dont know environment model, Monte-Carlo / Temporal Difference methods provided algorithms that are model-free to predict value functions.</p>

<h3 id="monte-carlo">Monte-Carlo</h3>

<p>Monte-Carlo(MC) methods learn directly from episodes of experience, instead of evaluate policy by expected return(based on environment knowledge), it uses mean return(based on empirical knowledge) to estimate the value function.</p>

<p>The basic idea is, to evaluate state value for \(s\), the firstt/every time \(t\) when state \(s\) is visited in an episode, increment counter \(N(s) \leftarrow N(s) +1\), increment total return \(S(s) \leftarrow S(s) + G_t\), the estimated state value can be calculated as:</p>

\[V(s) = \frac{S(s)}{N(s)}\]

<p>By law of large numbers, \(V(s) \rightarrow v_\pi(s)\) as \(N(s) \rightarrow \infty\).</p>

<p>By incremental MC updates, the final MC evaluation equation can be written as:</p>

\[V(s_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))\]

<h3 id="temporal-difference">Temporal Difference</h3>

<p>Temporal Difference(TD) method learns from incomplete eepisodes, in which the agent do not have to wait until finish whole episode to update value function, like did in MC. TD updates value function by leverage the differences between target and estimation in different time step. It uses idea of <strong>Bootstrapping</strong> with a biased estimation. Less precise than MC, but more convinent and with lower variance.</p>

<p>In Monte-Carlo methods, the value function is updated by:</p>

\[V(s_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))\]

<p>Alternatively in TD method, we update value of \(V(S_t)\) towards estimated return \(R_{t+1} + \gamma V(S_{t+1})\):</p>

\[V(s_t) \leftarrow V(S_t) + \alpha ([R_{t+1} + \gamma V(S_{t+1})] - V(S_t))\]

<p>Where \(R_{t+1} + \gamma V(S_{t+1})\) called <em>TD target</em>, and \(\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\) called <em>TD error</em>.</p>

<p>TD can learn before(or without) knowing the final outcome, whereas in order for MC to learn, we need to wait until the termination of the episode which only works in episodic environments.</p>

<h3 id="on-policy-value-based-controls">On-Policy Value-based Controls</h3>

<p><strong>MC based: Greedy Policy Improvements</strong></p>

<p>Evaluate state-action value functions \(q_\pi(s,a)\) instead of state value function \(v_\pi(s)\):</p>

\[q(s_t,a_t) \leftarrow q(s_t,a_t) + \alpha (G_t - q(s_t,a_t))\]

<p>Improve policy by \(\epsilon\)-greedily selecting \(q(s,a)\).</p>

<p><strong>TD based: Sarsa</strong></p>

<p>Sarsa algorithm, replace MC by TD in control loop:</p>

\[q(s_t,a_t) \leftarrow q(s_t,a_t) + \alpha (R + \gamma q(s_{t+1},a_{t+1}) - q(s_t,a_t))\]

<h3 id="off-policy-value-based-controls">Off-Policy Value-based Controls</h3>

<p>Objective:</p>

<ul>
  <li>Learn from observing humans or other agents.</li>
  <li>Re-use experience generated from old policies \(\pi_1;\pi_2,....,\pi_{t-1}\).</li>
  <li>Learn about optimal policy while following exploratory policy.</li>
  <li>Learn about multiple policies while following one policy.</li>
</ul>

<p><strong>MC based: Importance Sampling</strong></p>

<p>Gater trajectories from another policy distribution to update current distribution using a trick namely:</p>

<p>[Importance Sampling]</p>

<p>Estimate the expectation of distribution Q from P:</p>

\[\mathbb{E}_{X \backsim P}[f(x)] = \sum P(x) f(x) = \sum Q(x) \frac{P(x)}{Q(x)} f(x) = \mathbb{E}_{X \backsim Q}[\frac{P(x)}{Q(x)}f(x)]\]

<p>Define \(G_t^{\pi/\mu} = \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} \frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})} ... \frac{\pi(A_T|S_T)}{\mu(A_T|S_T)} G_t\), update value towards corrected return:
\(V(S_t) \leftarrow V(S_t) + \alpha(G_t^{\pi/\mu} - V(S_t))\)</p>

<p>Note that importance sampling can dramatically increase variance. This mechanism can also be applied to TD:</p>

\[V(S_t) \leftarrow V(S_t) + \alpha( \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} (R_{t+1} + \gamma V(S_{t+1})) - V(S_t))\]

<p><strong>TD based: Q-Learning</strong></p>

<p>Instead of using target value based on current policy \(\pi\), the target value in Q-Learning beased on greedy policy over state-action value function:</p>

\[q(s_t,a_t) \leftarrow q(s_t,a_t) + \alpha (R + \gamma \ max_a q(s_{t+1},a) - q(s_t,a_t))\]

<p><br /></p>

<h2 id="model-free-policy-based-methods"><strong>Model-free Policy Based methods</strong></h2>

<p>Advantages:</p>
<ul>
  <li>Better convergence properties.</li>
  <li>Effective in high-dimensional or continuous action spaces.</li>
  <li>Can learn stochastic policies.</li>
</ul>

<p>Disadvantages:</p>
<ul>
  <li>Typically converge to a local rather than global optimum.</li>
  <li>Evaluating a policy is typically inefficient and high variance.</li>
</ul>

<h3 id="gradient-based-policy-gradient">Gradient Based: Policy Gradient</h3>
<p>Define that in a MDP environment, total reward gain from a certain policy \(\pi\) can be shown as:</p>

\[\bar{R}_\theta = \sum_\tau R(\tau)p_\theta(\tau) \ \ \ _{(1)} \\
where \ \ p_\theta = p(s_1) \prod_{t=1}^{T} p_\theta(a_t \ given \ s_t) p(s_{t+1}|s_t,a_t) \ \ \ _{(2)}\]

<p>In which policy \(\pi\) is parameterised by \(\theta\), and \(\tau\) represents a single trajectory, \(p_\theta\) is the probability of the trajectory.</p>

<p>Since equation (1) involves reward of trajectory \(R(\tau)\) times the probability of trajectory \(\tau\) following policy \(\theta\), noted as \(p_\theta\), as well as the summing operation \(\sum_\tau\), it can be seen as the expectation of reward for a certain policy:</p>

\[\mathbb{E}_{\tau \backsim p_\theta(\tau)}[R(\tau)]\]

<p>Hence we want to maximize the expectation of reward for a certain policy, to achive this, calculate the gradient of the function and perform gredient acsent:</p>

\[\begin{aligned}
    \nabla_\theta \mathbb{E}_{\tau \backsim p_\theta(\tau)}[R(\tau)] = \nabla_\theta \int_{\tau_t} R_t \ p_\theta(\tau_t) \ \  d\tau_t \\ 
    = \int_{\tau_t} R_t \ \nabla_\theta \ p_\theta (\tau_t) \ \  d\tau_t \\ 
    = \int_{\tau_t} R_t \ p_\theta(\tau_t) \ \nabla_\theta \ log p_\theta (\tau_t) \ \  d\tau_t \\   
    = \mathbb{E}_{\tau \backsim p_\theta(\tau)}[R_t \ \nabla_\theta \ log p_\theta (\tau_t)]  

\end{aligned}\]

<p>\(\mathbb{E}_{\tau \backsim p_\theta(\tau)}[R_t \ \nabla_\theta \ log p_\theta (\tau_t)]\) can be approximated by collecting experience as much as possible and compute the average:</p>

\[\mathbb{E}_{\tau \backsim p_\theta(\tau)}[R(\tau) \ \nabla_\theta \ log p_\theta (\tau)] \approx \frac{1}{N} \sum_{n=1}^N R(\tau^n) \ \nabla_\theta \ log p_\theta (\tau^n)\]

<p>Since we dont have full model for \(p_\theta\), it is not possible to compute equation (2), that is, we dont know \(p(s_{t+1} \ given \ s_t,a_t)\), because this term depends on the environment. Here for gradient acsent with respect to \(\theta\), we only need \(\nabla log \ p_\theta(\tau_t)\) instead of the value of \(log \ p_\theta(\tau_t)\) itself, therefore simply replace \(p_\theta(\tau_t)\) by \(\pi_\theta(a_t \ given \ s_t)\) we get:</p>

\[\mathbb{E}_{\tau \backsim p_\theta(\tau)}[R(\tau) \ \nabla_\theta \ \sum_{t=1}^{T_n} log \ \pi_\theta(a_t | s_t)] \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n \ given \ s_t^n)\]

<p>After obtaining the gradient of objective function, policy parameters \(\theta\) are updated by gradient acsent:</p>

\[\theta \leftarrow \theta + \alpha \nabla \bar{R}_\theta

\\

where \ \ \nabla \bar{R}_\theta = \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n \ given \ s_t^n)\]

<p><strong>Tips1-Add a Baseline</strong></p>

<p>It is possible that in a specific reinforcement learning environment, that \(R(\tau^n)\) is always positive. In this case, we might monotoniclly increase the probability of a certain action, this can be solved by adding a baseline to our equation, so that instead of naively taking rewards feedback from environment, we compare it to the average rewards we have and make the reward be relative to all previous rewards:</p>

\[\nabla \bar{R}_\theta = \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} (R(\tau^n)-b) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n) \\
where \ \ b \approx \mathbb{E}[R(\tau)]\]

<p>In other words, instead of rewarding trajectory by only the environment rewards, we reward a trajectory by how looking at how good this trajectory is, comparing with all other collected trajectories, since all actions in a same trajectory are being weighted by same reward, yet those actions might benefits for different amount.</p>

<p>To address this, we weight the \(a_t\) by the reward obtained from time \(t\), add a discount factor to rewards obtained in later stages.</p>

\[\nabla \bar{R}_\theta = \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} ([\sum_{t^` = t}^{T_n} \gamma^{(t^`-t)} \cdot r_{t^`}^n ] -b) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n) \\
where \ \ b \approx \mathbb{E}[R(\tau)]\]

<p><strong>Tips2-Assign Suitable Credit</strong></p>

<p>The current version of objective function evaluates the whole trajectory by the total rewards obtained from the environment, it is reasonable, but assumes in-precise correlations between each actions in the trajectory.</p>

<p><br /></p>

<h2 id="actor-critic-integrating-value-based--policy-based"><strong>Actor-Critic: Integrating Value-based &amp; Policy-based</strong></h2>

<p>The above PG(Policy Gradient) algorithm is evaluating the policy by MC-style critic(i.e. mean expected reward returned by the environment), in Actor-Critic, we define a critic:</p>

\[Q_w(s,a) \approx Q^{\pi_\theta}(s,a)\]

<p>Where the critic approximates state-action value function \(Q(s,a)\), the actor approximates the policy \(\pi\), there are parameterized by \(w\) and \(\theta\) respectively.</p>

<p>Actor-critic algorithm follow an approximate policy gradient, the actor network can be updated by:</p>

\[\nabla_\theta J(\theta) \approx \mathbb{E}_{\pi_\theta}[\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)] \\

\nabla \theta = \alpha \nabla_\theta log \pi_\theta(s,a) Q_w(s,a) \\

\theta_{t+1} \leftarrow \theta_t + \lambda \nabla\theta_t\]

<p>The critic network is based on critic functions, here use Q-function as an example, the critic network can be updated as:</p>

\[\nabla_w J(w) \approx \mathbb{E}_{\pi_\theta}[MSE(Q_{t},R_{t+1}+max_a Q_{t+1})] \\

\nabla w = MSE(Q_{t},R_{t+1}+max_a Q_{t+1}) \\

w_{t+1} \leftarrow w_t + \lambda \nabla w_t\]

<p>Instead of leting critic to estimate state-value function, we can allow it to alternatively estimates Advantage function \(A(s,a) = Q_w(s,a) - V_v(s)\) to reduce the variance. There are many alternative critic function choices.</p>

<p><br /></p>

<h2 id="continuous-action-space"><strong>Continuous Action Space</strong></h2>

<p>Methods proposed so far only solves for environments that are with discrete action space, so that value functions for each actions or the probability distribution of selecting actions could be computed. However, in real world, most of the problem are with continous action space.</p>

<h3 id="deep-derministic-policy-gradient">Deep Derministic Policy Gradient</h3>

<p>Deep Derministic Policy Gradient is then proposed, it is able to solve RL environments with continuous action space by integrating ideas of DQN and PG. It can be viewed as extended version of DQN that is able to solve problems with continous action space. DDPG algorithm uses Actor-Critic architecture.</p>

<p>In DQN, in order to evaluate value function, we need \(max_aQ_{t+1}\), it is not possible to compute such value in continous action space. Instead of inputing only the state into critic network and obtain the Q-values for all actions, DDPG critic network takes next action computed from actor network as well, and evaluate Q value for this certain action. Updating of DDPG critic network is the same to DQN:</p>

\[\nabla_w J(w) \approx \mathbb{E}_{\pi_\theta}[MSE(Q_{t},R_{t+1}+ Q_{t+1}^{a \backsim Actor})] \\

\nabla w = MSE(Q_{t},R_{t+1}+Q_{t+1}^{a \backsim Actor}) \\

w_{t+1} \leftarrow w_t - \lambda \nabla w_t\]

<p>Intuitively, in DDPG, the actor network performs differently as the one in PG, it is not possible for it to compute probalibity distributions for all actions in continous action space, therefore we alter the network to output a certain action that could be with max Q value. The target of the actor network in DDPG is to maximize the value of \(Q_t(s,a)\) evaluated by critic network, therefore to update actor network, we use gradient acsent:</p>

\[pg = \frac{\partial Q(s,\pi(s;\theta),w)}{\partial \theta} = \frac{\partial Q(s,a,w)}{\partial a} \cdot \frac{\partial a}{\partial \theta} \\

\theta \leftarrow \theta + \bar{\lambda} \theta\]

<p>Note that in DDPG, tricks like target networks for both AC networks; memory buffer are being used. We also add a environmental noise \(N\) when performing actions to allow exploration, as well as off-policy learning.</p>

<h3 id="proximal-policy-optimization">Proximal Policy Optimization</h3>

<p>Baseline algorithm of OpenAI. PPO allows off-policy learning to policy gradient algorithm. In policy gradient, we update our policy network by compute gradient of the expected reward function with respect to policy parameters \(\theta\), and perform gradient acsent to maximize it:</p>

\[\nabla \bar{R}_\theta = \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n) \\
= \mathbb{E}_{\tau \backsim \pi_\theta}[R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n | s_t^n)] \\
\theta \leftarrow \theta + \alpha \nabla \bar{R}_\theta\]

<p>In PPO algorithm, instead of sampling trajectories from policy \(\pi_\theta\), in order to increase sample efficiency(reuse expirence), we sample trajectories from another policy \(\pi_{\theta'}\) and apply a importance sampling method to correct the difference.</p>

\[\nabla \bar{R_\theta} =  \mathbb{E}_{\tau \backsim \pi_{\theta'}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta'}(a_t|s_t)} R(\tau^n) \ \nabla_\theta \ log \ \pi_\theta(a_t^n given s_t^n)]\]

<p>In addition, we need to add a regularzation term (or in TRPO, add a constrain) to the objective function to constrain the difference between two distributions, therefore the objective function becomes:</p>

<p>\(J(\theta) = J_{\theta'}(\theta) - \beta KL(\theta,\theta')\)
Where adaptively set the value of \(\beta\), specificlly when \(KL(\theta,\theta') &gt; KL_{max}\), increase \(\beta\); when \(KL(\theta,\theta') &lt; KL_{min}\), decrease \(\beta\).</p>

<p><br /><br /></p>]]></content><author><name></name></author><category term="studynote" /><category term="images," /><category term="links," /><category term="math" /><summary type="html"><![CDATA[Reinforcement Learning Note. [Model-based | Model-free Value-based | Model-free Policy-based | AC | Continuous Action Space]]]></summary></entry><entry><title type="html">Study on Efficient Models 2</title><link href="https://huskykingdom.github.io/blog/2022/em2/" rel="alternate" type="text/html" title="Study on Efficient Models 2" /><published>2022-10-28T15:59:00+00:00</published><updated>2022-10-28T15:59:00+00:00</updated><id>https://huskykingdom.github.io/blog/2022/em2</id><content type="html" xml:base="https://huskykingdom.github.io/blog/2022/em2/"><![CDATA[<p>Download full version of this notes with more details and images <a href="/assets/pdf/Notes.pdf">here</a>.</p>

<h2 id="efficientnet-v1-2019">EfficientNet V1 (2019)</h2>
<h3 id="idea--background">Idea &amp; Background</h3>
<p>The paper[0] proposed that, modern CNNs were developed with more layers(deeper), more channels(wider), and higher quality of input images(hgher resolutions). However, scaling up any of the parameters mentioned monotonically to a large number would not very much benefits the model, in particular, the model might in this case, reaches an accuracy saturation.</p>

<p>The paper evaluated CNN models that monotonically scalling up its width $w$, depth $d$ and resolution $r$, the improvements is significant until the scaling reaches a certain limit.</p>

<p>The paper concludes that:</p>

<ul>
  <li><strong>Observation 1</strong> – Scaling up any dimension of network
width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.</li>
</ul>

<p>Intuitively, increased input resolutions needs wider networks that are able to capture more fine-grained patterns with more pixels, as well as the higher depth such that the larger receptive fields would help capture similar features that include more pixels.</p>

<p>These result lead us to the second observation:</p>

<ul>
  <li><strong>Observation 2</strong> – In order to pursue better accuracy and
efficiency, it is critical to balance all dimensions of network
width, depth, and resolution during ConvNet scaling.</li>
</ul>

<p>Since the current existing methods are adjusting width, depth, resolutions manually, the paper hence propose an new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective <strong>compound coefficient</strong>.</p>

<h3 id="compound-coefficient-scaling">Compound Coefficient Scaling</h3>

<p>Define a compound coefficient \(\phi\) that is used to uniformly scales network width, depth, and resolution in a principled way:</p>

<p>depth: \(d = \alpha^\phi\)</p>

<p>width: \(w = \beta^\phi\)</p>

<p>resolution: \(r = \gamma^\phi\)</p>

<p>s.t. \(\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\)</p>

\[\alpha \ge 1, \beta \ge 1, \gamma \ge 1\]

<p>From which, the \(\alpha,\beta,\gamma\) are constants that can be determined by a small grid search, \(\phi\) is a user-defined coefficient that controls how many more resources are available for model scalling.</p>

<p>The paper point out that, the FLOPSof a regular convolution operation is proportional to \(d,w^2,r^2\), that is in other words, 2X network depth will gain 2X FLOPS, but 2X network width or resolution will increase FLOPS by 4X. Also, because convolutional layers are usually dominate the computation cost in ConvNets, scalling a ConvNet with above equation will approximately increase total FLOPS by \((\alpha \cdot \beta^2 \cdot \gamma^2)^\phi\), the method constraints \(\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\) so that the total FLOPS will approximately increase by \(2^\phi\).</p>

<h3 id="efficientnet-architecture">EfficientNet Architecture</h3>

<p>The EfficientNet is based on MnasNet (by Platform-aware Neural Architecture Search), except EfficientNet-B0 is slightly bigger with FLOPS targets set to 400M.</p>

<p>In which, the MBConv block is mobile inverted bottleneck, the SE block is added into each block for optimization.</p>

<p>Starting with EfficientNet-B0, the compound scaling method is performed with 2 steps:</p>

<ul>
  <li>STEP 1: first fix φ = 1, assuming twice more resources available, and do a small grid search of \(\alpha,\beta,\gamma\)
based on equation shown above. In particular, the paper found
the best values for EfficientNet-B0 are \(α = 1.2\), \(β =
1.1\), \(γ = 1.15\), under constraint of \(α · β^2 · γ^2 ≈ 2\).</li>
  <li>STEP 2: fix \(\alpha,\beta,\gamma\) as constants and scale up baseline network with different \(\phi\) using above equation, to obtain EfficientNet-B1 to B7.</li>
</ul>

<h2 id="efficientnet-v2-2021">EfficientNet V2 (2021)</h2>

<h3 id="background--idea">Background &amp; Idea</h3>
<p>EfficientNet V2 has got improved training speed and better performance than EfficientNet V1. In this upgraded version, we focus not only on the accuracy and #parameters/FLOPs, but jointly focusing on the training efficiency as well.</p>

<p>The paper[1] identifies several problems of the previouse EfficientNet V1:</p>

<ul>
  <li>Training with very large image sizes is slow. <strong>(Proposed Solution: Progressive Learning)</strong></li>
  <li>Depthwise Convolutions are slow in early layers but effective in later layers, since the depthwise convolutions often cannot fully utilize modern accelerators. <strong>(Proposed Solution: Replacing MBConv layers by  Fused-MBConv via NAS)</strong></li>
  <li>Equally scaling up every stage is sub-optimal. <strong>(Proposed Solution: New Scaling Rule and Restriction)</strong></li>
</ul>

<p>The Fused-MBConv (better utilize mobile or server accelerators) is replacing the original expension layer and depthwise convolution layer of MBConv by a single 3x3 convolution, the paper evaluated that by using Fused-MBConv in early stage(1-3) helps accelerate the training step with a small overhead on parameters and FLOPs. NAS is used to automatically search for the best combination.</p>

<h3 id="training-aware-nas-and-scaling">Training-Aware NAS and Scaling</h3>
<p>The Training-Aware NAS is based on Platform-Aware NAS[2], which its search space is also a stage-based factorized space, with the following search options:</p>

<p>Convolution Ops : {MBConv, Fused-MBConv}</p>

<p>Kernel Size: {3x3, 5x5}</p>

<p>Expension Ratio: {1,4,6}</p>

<p>However, in Training-Aware NAS, the paper point out that, they removed unnecessary search options like skip ops, and resued the same channel sizes from the backbone as they aree already searched. In addition, the search reward in Training-Aware NAS conbines model accuracy \(A\), the normalized traning step time \(S\), and the parameter size \(P\), by using a simple weighted product \(A \cdot S^w \cdot P^v\), where \(w=-.0.07\) and \(v=-0.05\),  empirically determined to balance the trade-offs similar to [2].</p>

<p>As for the scaling part, EfficientNetV2-S is scaled up to EfficientNetV2-M/L using compound scaling with optimizations: (1) Restrict maximum inference image size to 480. (2) Gradually add more layers to later stages to increase the network capacity without adding much runtime overhead.</p>

<h3 id="progressive-learning">Progressive Learning</h3>

<p>The main idea of progressive learning is to increase image size and regularzation magnitude at the same time during the training stage. The paper agure that the loss of accuracy of only progressively enlarge input image size during training drops due to unbalanced regularization.</p>

<h2 id="references">References</h2>

<p>[0] Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR</p>

<p>[1] Tan, M., &amp; Le, Q. (2021, July). Efficientnetv2: Smaller models and faster training. In International Conference on Machine Learning (pp. 10096-10106). PMLR.</p>

<p>[2] Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., &amp; Le, Q. V. (2019). Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2820-2828).</p>

<p>[3] Brock, A., De, S., Smith, S. L., &amp; Simonyan, K. (2021, July). High-performance large-scale image recognition without normalization. In International Conference on Machine Learning (pp. 1059-1071). PMLR.</p>]]></content><author><name></name></author><category term="studynote" /><category term="images," /><category term="links," /><category term="math" /><summary type="html"><![CDATA[Walkthrough more SOTA compact models.]]></summary></entry><entry><title type="html">Study on Efficient Models</title><link href="https://huskykingdom.github.io/blog/2022/em1/" rel="alternate" type="text/html" title="Study on Efficient Models" /><published>2022-10-07T15:59:00+00:00</published><updated>2022-10-07T15:59:00+00:00</updated><id>https://huskykingdom.github.io/blog/2022/em1</id><content type="html" xml:base="https://huskykingdom.github.io/blog/2022/em1/"><![CDATA[<p>Download full version of this notes with more details and images <a href="/assets/pdf/M3.pdf">here</a>.</p>

<h2 id="efficient-methods">Efficient Methods</h2>
<p>The energy is mainly consumed in DNNs by data transferring(in memory), and MAC(Multiplication &amp; Accumulation) operations. In order to reduce the energy consumption, we need methods or models that reducing the cost of memory and compute in DNNs, that is, the Neural Network Compression.</p>

<p>According to recent paper surveries [0][1][3], we can classifies the current existing efficient compression methods by their different objectives in the following way:</p>

<table>
  <thead>
    <tr>
      <th>CLASS</th>
      <th>OBJECTIVE</th>
      <th>MAJOR METHODS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Compact Model</td>
      <td>Design smaller base models that can still achive acceptable accuracy.</td>
      <td>Efficient Lightweight DNN Models</td>
    </tr>
    <tr>
      <td>Tensor Decomposition</td>
      <td>Decompose bloated original tensors into more smaller tensors.</td>
      <td>–</td>
    </tr>
    <tr>
      <td>Data Quantization</td>
      <td>Reduce the number of data bits.</td>
      <td>Network Quantization</td>
    </tr>
    <tr>
      <td>Network Sparsification</td>
      <td>Sparse the computational graph(number of connection/neurons).</td>
      <td>Network Pruning</td>
    </tr>
    <tr>
      <td>Knowledge Transferring</td>
      <td>Learning output distributions of trained large model.</td>
      <td>Knowledge Distillation</td>
    </tr>
    <tr>
      <td>Architecture Optimazation</td>
      <td>Optimaze the model parameters.</td>
      <td>NAS, Genetic Algorithm</td>
    </tr>
  </tbody>
</table>

<h2 id="compact-model"><strong>Compact Model</strong></h2>
<p>Mordern DNNs` performence impovement is mainly driven by deeper and wider networks with increaseed parameters and operations(MACs)[0], the compact models tends to reduce overhead while maintaining accuracy as much as possible.</p>

<p>Compact models try to design smalleer based models by allowing deeper/wider networks with expanded FMs, more complex branch topology, and more flexible Conv arithmetic. They can be classified based on the following two aspects:</p>

<ol>
  <li>Ensure large spatial correlations(Receptive Field) while remains compact.</li>
  <li>Ensure deeper channel correlations while remains compact.</li>
</ol>

<h2 id="overview-on-current-dnn-models">Overview on Current DNN Models</h2>
<p>I have taken an overview on most of the popular models that are proposed in the DNN field, all of them have different characteristics over network accuracy, speed, and size. Taken the example from the paper[0] which summerize the most frequently used DNN models:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/efficient_imgs/21.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>From above illustrated models, the following DNN models are typically being referred as <strong>Lightweight Models</strong>:</p>

<ul>
  <li>SqueezeNets</li>
  <li>ShuffleNets</li>
  <li>MobileNets</li>
  <li>GoogleNet (Inception)</li>
  <li>EfficientNets</li>
  <li>NasNets</li>
  <li>DenseNets</li>
</ul>

<p>…</p>

<p>I will then evaluate each of them by understanding the foundamentals of the idea, implementations, as well as the comparisons &amp; commonalities between them.</p>

<h2 id="mobilenets">MobileNets</h2>
<h3 id="main-idea">Main Idea</h3>
<p>Paper presented on 2017 by Google [3]. Lightweighted Deep Convolutional Neural Network with drastic number of parameters and calculation operations drop while maintaining reasonable accuracy. Targeting to deploy or train on edge devices with less power consumption, as well as the model size, together with faster inference time (latency).</p>

<p>The main idea used in MobileNet is construct relatively compact deep neural networks with use a form of factorized convolutions, namely <strong>Depthwise Separable Convolution</strong>, to enable both spitial correlations and channel correlations.</p>

<p>Different from the conventional convolution operations, Depthwise Separable Convolution is performed as follows:</p>

<ol>
  <li>[Depthwise Convolution] Apply one single filter to each channel of input featuree map. Ensures the spatial correlation.</li>
  <li>[Pointwise Convolution] Apply filter of size 1x1 to the combined output of the Depthwise Convolution. Ensures the channel correlation.</li>
</ol>

<p>Depthwise Separable Convolution performs convolution channel by channel, then combines the resulting tensors by using Nx1x1x(#Channel) fileters to produce output. Saving dramastic parameters comparing with the standard method.</p>

<h3 id="mobilenet-v2"><strong>MobileNet V2</strong></h3>
<h3 id="theory">Theory</h3>
<p>Intuitively, the computation &amp; memory cost of DNN models depend largely on the input resolutions and the number of tensors channels of the inner computing graph. Conventional convolution methods extracts feature information by internal Conv kernels that map the input into higher dimensions(channels) to allow the original image to form a “manifold of interest”[3] we wish to learn, by set of those inner-layer activations. Since it has been long assumed that manifolds of interest in DNNs could be embedded in low-dimensional subspaces, in order to design compact models, we can reduce the operation space by simply reducing the dimensionality of the layers, while remaining the shape of the “manifold of interest”.</p>

<p>MobileNet V1 uses width multiplier/resolution multipliers[2] to allow tradeoffs between computation and accuracy. Following above intuition, those multipliers allows one to reduce the dimensionality until the manifold of interests spans the entire space[3].</p>

<p>However, due to the non-linear property of Conv layers, the “manifold of interests” may collapse. The paper[3] gives an intuitive illustration of this idea by embedding low-dimensional manifolds into n-dimensional spaces.</p>

<p>It is can be observed that the information losses a lot when low-dimentional(e.g. when n=2,3) manifold is transformed with \(ReLU\); whereas the information mostly reserved when high-dimentional manifold is transformed with \(ReLU\).</p>

<p>This result suggesting use more dimensionalities as possible, and it is conflicting with the idea of lightweighted models including MobileNet V1.</p>

<p>MobileNet V2 is trying to address this problem, preventing the information loss, as well as remaining even lower parameters/memory usage.</p>

<h3 id="main-idea-1">Main Idea</h3>
<p>The insights used for MobileNet V2 architecture can be views as the follows:</p>

<ul>
  <li><strong>Depthwise Separable Convolution</strong> - Saving parameter and operation complexities.</li>
  <li><strong>Linear Bottlenecks</strong> - Using the partial non-linearity property of \(ReLU\) function, insert a linear transformation after high-dimensional \(ReLU\) transformation to address the conflict of complexity and manifold collapsion.</li>
  <li><strong>Inverted Residuals</strong> - More memory efficient to shortcut connections between linear layers. (When stride=2 this can be optional according to the paper.)</li>
</ul>

<p>This design is named the <strong>Bottleneck Residual Block</strong>, with a 1x1 filter performing transformation to increase the dimensionality of input activated by \(ReLU6\), then perform 3x3 depthwise convolution, subsequently using another 1x1 filter to perform linear transformation to produce the output, reduce the dimensions as well as preventing lose of information. Lastly, add shortcut connections between consecutive linear layers to improve the ability of a gradient to propagate across layers. Each layer is followed by a batnormalization.</p>

<h3 id="mobilenet-v3"><strong>MobileNet V3</strong></h3>
<h3 id="main-idea--background-knowledge">Main Idea &amp; Background Knowledge</h3>

<p>Proposed by Google in 2019. The MobileNet V3 achives better performance, with \(3.2\%\) more accurate on ImageNet classification while reducing latency by \(20\%\), in comparison to previous MobileNet V2. It is mainly constructed from the following ideas:</p>

<ul>
  <li>Bottleneck Block with SENet block introduced.</li>
  <li>2 Neural Architecture Search algorithms are used, for block optimization and layer optimization.</li>
  <li>Redesigned some of the redundent expensive structures.</li>
  <li>New Nonlinearities.</li>
</ul>

<p>In order to understand the design details of MobileNet V3, we need to firstly take a look at Mnasnet[5] &amp; Netadapt[6] &amp; SENet[8].</p>

<hr />
<p><strong>Mnasnet: Platform-Aware NAS for Mobile [5]</strong>
___
Proposed by Google in 2019, Mnasnet is an Neural Architecture Search guilded auto designed model, its main conribution is proposed that the conventional FLOPS proxy used widely in NAS methods are not the accurate approximation of the model lantency, it novelly used the real lantency measuremeants from running model on physical devices and thus guild the design.</p>

<p>The RNN controller firstly sample model parameters from the search space, then the trainer excutes with selected parameters, and accordingly evaluate the accuracy of the current model, the model is subsequently passed into real mobile phones to obtain the real lantency, together with model accuracy and latency, compute the manually definded reward and feedback to the controller.</p>

<p>The objective function for Plateform-Aware NAS is definded as:
\(\max_{m} \quad ACC(m)\\
w.r.t. \quad LAT(m) \le T\)
However, given the computational cost of performing NAS, we are more interested in finding multiple Pareto-optimal solutions in a single architecture search, instead of maximize a single metric.</p>

<p>Plateform-Aware NAS uses a customized weighted product method to approximate Pareto optimal solutions, the goal is defined to:
\(\max_{m} \quad ACC(m) \times [\frac{LAT(m)}{T}]^w\)
In which the \(w\) is the weight factor defined as:
\(w = \begin{cases}
\alpha &amp; LAT(m) \le T \\
\beta &amp; otherwise
\end{cases}\)
Empirically, the paper suggests that doubling the latency usually brings about 5% higher accuracy gain, by idea of obtaining Pareto-optimal, given two models, (1) M1 has latency \(l\) and accuracy \(a\), (2) M2 has latency \(2l\) and accuracy \(1.05a\), they should have similar reward, definded as follows:
\(Reward(M1)=a·(\frac{l}{T})^\beta \approx Reward(M2)=a·(1+5\%)·(\frac{2l}{T})^\beta\)
Solving for above equation gives \(\beta \approx -0.07\), Plateform-Aware NAS uses \(\alpha = \beta = -0.07\) in their experiments unless explicitly stated.</p>

<hr />
<p><strong>Netadapt: Platform-aware neural network adaptation for mobile applications [6]</strong>
___
Proposed by Google in 2018. Netadapt is an layer by layer network compression algorithm, which alters the number of filters in each layer to obtain given resource budget, for a certain pre-trained model.</p>

<p>It is propsed in the pape, that similar to [5], the traditional measurements of #parameters and #MACs might not be sufficient to conclude the latency and energy consumption. Netadapt also uses the direct metric to guild the filters pruning.</p>

<p>One main difference between Netadapt filters pruning and energy-aware pruning[7] is that Netadapt uses empirical matric per layer to estimate the real resouce consumption, so that no further detailed lower-level knowledge is required for estimating the real matrics.</p>

<p>The problem can be formulated as the following:
\(\max_{Net} \quad Acc(Net) \\
w.r.t. \quad Res_j(Net_i) \le Bud_j, \ j \ = \ 1,...,m\)
Whereas in Netadapt, considering maintaining the accuracy needs re-train for each alternation step, it breaks above problem into following series of easier problems and solves it iteratively:
\(\max_{Net_i} \quad Acc(Net_i) \\
w.r.t. \quad Res_j(Net_i) \le Res_j(Net_{i-1}) - \Delta R_{i,j}, \ j \ = \ 1,...,m\)
Where \(\Delta R_{i,j}\) is called “Resource Reduction Schedule”, similar to the concept of learning rate schedule, It is an hyper-parameter stands for the reduction step size for each iteration.</p>

<p>The algorithm iteratively find solutions that within current resource budget in each iteration, layer-by-layer, perform ShortTermFineTune after each filter-pruned layer. Each time a layer is pruned, a new network with only that specific layer is pruned is generated and stored.</p>

<p>After all the layers have been evaluated, select one network with the highest accuracy from the stored \(K\)(Equal to number of layers) networks.</p>

<p>Repeat the process until the budget limitation is satisified.</p>

<hr />
<p><strong>Squeeze-and-Excitation Networks [8]</strong>
___</p>

<p>The paper introduced a method to emphasis the channel-wise features and their correlations by “Attention” mechanism. In which for a convolution output \(U=X*F_{tr}\), where \(X\) is the input and \(F_{tr}\) is the standard convolution operation, we can plug in behind an SE block to boost feature discriminability.</p>

<p>Such design won the first place in the ILSVRC 2017 classification competition with top performing model ensemble achieves a \(2.251\%\) top-5 error on the test set, that is \(\approx25\%\) relative improvement compared with previous winner.</p>

<hr />

<p>Now back to MobileNet V3. There are two types of MobileNet V3 models according to the paper, the MobileNetV3-Large &amp; MobileNetV3-Small, targeted at high and low  resource use cases respectively.</p>

<h3 id="block-structure">Block Structure</h3>

<p>The MobileNet V3 integrates lightweight attention module <strong>Squeeze and Excitation</strong> into its bottleneck block structure, where the SE block is placed after depthwise convolution inthe expansion for attention to be applied on the largest representation. As can be observed from above. Furthermore, MobileNet V3 uses compression rate as \(\frac{1}{4}\) in fully connected layers, by expiriments, doing so increases the accuracy at the modest incrase of number of parameters, and with no discernible latency cost.</p>

<h3 id="network-search">Network Search</h3>

<p>1 - Block-wise Search</p>

<p>MobileNet V3 used <strong>Platform-aware Neural Architecture Search</strong> approach for large mobile models, and found the similar results as in [5], therefore MobileNet V3 simply reuse the same MnasNet-A1 as initial large mobile model.</p>

<p>However, by observations that small mobile is more lantency-sensitive, in other words the model accuracy changes more dramatically with lantency for small models, the original assumption made in [5] for the empirical accuracy-lantency rate might not be suitable. Therefore, MobileNetV3-Small uses weight factor \(w=-0.15\) insteead of \(w=-0.07\).</p>

<p>2 - Layer-wise Search</p>

<p>After the rough blocks architecture is defined, MobileNet V3 then uses <strong>NetAdapt</strong> as complimentary to search for optimal individual layer configurations in a sequential manner, rather than trying to infer coarse but global architecture.</p>

<table>
  <tbody>
    <tr>
      <td>MobileNet V3 has modifed the algorithm by selecting the final proposals by one that maximize $$\frac{\Delta Acc}{</td>
      <td>\Delta Latency</td>
      <td>}\(, in which $\Delta Latency$ satisifies reduction schedule\)\Delta R$$. The intuition is that because our proposals are discrete, we prefer proposals that maximize the slope of the trade-off curve.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>By setting $$\Delta R = 0.01</td>
      <td>L</td>
      <td>\(and\)T=10000$$, where $L$ is the lantency of the original model, and $T$ is the number of iterations for excuting NetAdapt algorithm. Like did in [6], the proposals for MobileNet V3 are allowed from the following two types of altering:</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Reduce the size of any expansion layer;</li>
  <li>Reduce bottleneck in all blocks that share the same bottleneck size to maintain residual connections.</li>
</ul>

<h3 id="redesigning-expensive-layers">Redesigning Expensive Layers</h3>

<p>The current model based on MobileNet V2 uses 1x1 pointwise convolution as the final layer for dimension expansion, and then after Avg Pooling + anotehr 1x1 convolution, reduce the both channel size and feature map size to prodece output. The final dimension expansion layer is important as it ensures rich features for prediction, however, this is lantency-expensive.</p>

<p>MobileNet V3 moves this layer past the final average pooling. This makes the computation of the features becomes nearly free in terms of computation and latency.</p>

<p>By moving this layer after average pooling, the previous projection layer is no longer needed to reduce the computation, thus the projection layer and its 3x3 depth filtering layer(Replaced by Average Pooling) of the last bottleneck block are all removed.</p>

<h3 id="nonlinearities">Nonlinearities</h3>

<p>The piece-wise linear hard analog of $Sigmoid$ and $Swish$ activations is introduces in MobileNet V3, formally:
\(hard-Sigmoid[x] = \frac{ReLU6(x+3)}{6} \\ \\
and \\
hard-Swish[x] = x \frac{ReLU6(x+3)}{6} \\ \\\)
It is found by the expiriment that those hard functions have no discernible differences than the soft ones in accuracy, but they are:</p>

<ol>
  <li>More friendly in quantized mode since it eliminates potential numerical precision loss caused by different implementations of the approximate sigmoid.</li>
  <li>h-swish can be implemented as a pice-wise function to reduce the number of memory acesses driving the latency cost down substantially.</li>
</ol>

<p>In addition, the paper also mentioned that because the cost of applying nonlinearity decreases as we go deeper into the network, due to the reductions of resolution size which halves each layer activation memories everytime, MobileNet V3 only uses h-swish functions in deeper layers(Secound half of the layers). Even with this, the h-swish function still introduce some latency cost, this can be addressed by using optimized implementation based on a piece-wise function.</p>

<h2 id="inceptions">Inceptions</h2>
<h5 id="which-i-call-thee-width-learning-lol">(Which I call “Thee Width Learning” lol.)</h5>

<h3 id="googlenet--inception-v1"><strong>GoogLeNet &amp; Inception V1</strong></h3>

<p>GoogLeNet won the first place of 2014 ILSVRC chanllenge, first proposed by Google in 2014, variase impovements are being made afterwards, including InceptionV2(BN), InceptionV3, InceptionV4 and Xception.</p>

<h3 id="theory-1">Theory</h3>

<p>The paper[9] proposed that deeper(blocks)/wider(channels) DNN models are with large number of parameeters and thus more easily to be trained as overfitting model, where for high quality models, the training dataset could be very limited since preparing such dataset is not just trick, but expensive.</p>

<p>Another issue of those models is the increased computational cost and the memory cost, which is very energy-inefficient. The fundamental way of solving both issues that are widely used during that time is sparses the computational graph, however, since the lower-level hardware designs of our computing devices are mostly structured designed, sparse redundent weights randomly does not bring much benefits to the models. Structural Purning proposed later, however, is lossing the accuracy of DNN models.</p>

<p>The paper referenced the main result of [10], which states that:</p>

<hr />
<p><strong>If the probability distribution of the dataset is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.</strong>
<em>__
<strong>Neurons That Fire Together, Wire Together. –Hebbian Priciple</strong>
__</em>
Based on above, the Inception structure is proposed, with dense wider design to approximate local sparsity and aggregate the resulting chennels as the output.</p>

<p>In first attempt from 2(a), the Inception is consistes of combination of all state-of-art sparse layers with their output filter bancks concatenated into a single output. With 1x1 dimension reduction is applied, rather than the naive approach, the Inception block could be extracting more informative information (from difference sparse units + different scale spatial information with multi-kernel sizes), by using above designs, Inception is allowed to increaseing units at each block without computational complexity blowing-up, whilist keeping the lower-level sparse computational away.</p>

<p>An compact model using Inception blcoks is proposed in the paper, namely the GoogLeNet (With 2 additional auxiliary classifiers weighted 0.3 at lower/middle depth).</p>

<p>This design wins VGG by ~1.5% less error rate.</p>

<h3 id="inception-v2-batchnorm"><strong>Inception V2 (BatchNorm)</strong></h3>

<p>Proposed by Google in 2015 [11]. According the widely used SGD meethod for DNN weights updating, the paper points that the layer output is largely effected by the inputs from the previous layer. Therefore, during training/testing stage, the distributions of the inputs matters to the model. It would be advantageous for the distribution of the inputs remain fixed over time so the network weight does not have to readjust to adapt new distribution. Such distribution transformation of inter-network nodes is called <strong>Internal Covariate Shift(ICS)</strong>.</p>

<p>The paper proposed method called <strong>Batch Normalization</strong>, which is nowadays commonly used in DL community.</p>

<p>During the backpropagation, the scale parameter $\gamma$ and shift parameter $\beta$ also needs to be learned by passing through the loss $l$ and compute the gradient with respect to these parameters.
Batch Normalization also allows learning rate to be set higher without weights blowing-up. Occasionally speaking, with BN, the dropout layer is sometimes discared.</p>

<p>Inception V2 is based on Batch Normalization Method by adding BN after layer transformation, using $ReLU$ as activation functions, and replace the 5x5 Conv by two consecutive 3x3 Conv.</p>

<p>The Inception trained with different larger learning rate achives some result much faster interms of training episodes.</p>

<h3 id="inception-v3"><strong>Inception V3</strong></h3>

<p>Propsed by Google in 2015. [12] The paper relooked into the inception designs and propsed the following 4 General Design Principles regarding DNNs, that empirically:</p>

<ol>
  <li>Avoid extreme representational bottlenecks, especially in the early network. Otherwise information could be lost.</li>
  <li>Higher dimensional representations are easier to process locally within a network, hence converges faster.</li>
  <li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in rep- resentational power.</li>
  <li>Balance network width and depth.</li>
</ol>

<p>_</p>

<p>More efficient Inception could be achived by <strong>Factorizing the Convolution</strong>. First, factorize the original convolutions to the combinations of smaller combinations stacking on top of each other, by doing so, we made the in-block network deeper, hence imporved representation power of the non-linearity of the model, while significantly saving parameters and MACs.</p>

<p>Another approach to factorize the convolution, rather than replacing larger convolution by smaller convolutions, is to factorize the original convolution into asymmetric convolution.</p>

<p>Wherea kernel of 3x3 could be disolved into stacking of 1x3 and 3x1, with 33% less parameters.</p>

<p>In addition, the paper proposed an method of down-sampling. The current down-samplings are performed by either:</p>

<ol>
  <li>Pooling -&gt; Enlarge Dimensions: This will suffer lose of information.</li>
  <li>Enlarge Dimensions -&gt; Pooling: Computationally costy.</li>
</ol>

<p>The paper proposed a method called <strong>Efficient Grid Size Reduction</strong>, which leverage the speciality that Inception blocks have units, to parallely perform above actions to produce output.</p>

<p>Such approach achieves $7.8\%$ and $3.8\%$ less error rate compared with InceptionV1 and V2 respectively.</p>

<h2 id="references">References</h2>
<p>[0] Deng, Lei, et al. “Model compression and hardware acceleration for neural networks: A comprehensive survey.” Proceedings of the IEEE 108.4 (2020): 485-532.</p>

<p>[1] Cheng, Yu, et al. “A survey of model compression and acceleration for deep neural networks.” arXiv preprint arXiv:1710.09282 (2017).</p>

<p>[2] Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., … &amp; Adam, H. (2017). Mobilenets: Efficient convolutional neural networks for mobile vision applications.</p>

<p>[3] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., &amp; Chen, L. C. (2018). Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4510-4520).</p>

<p>[4] Howard, A., Sandler, M., Chu, G., Chen, L. C., Chen, B., Tan, M., … &amp; Adam, H. (2019). Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 1314-1324).</p>

<p>[5] Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., &amp; Le, Q. V. (2019). Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2820-2828).</p>

<p>[6] Yang, T. J., Howard, A., Chen, B., Zhang, X., Go, A., Sandler, M., … &amp; Adam, H. (2018). Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 285-300).</p>

<p>[7] Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne: Designing energyefficient convolutional neural networks using energy-aware pruning. In:
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
(2017)</p>

<p>[8] Hu, J., Shen, L., &amp; Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132-7141).</p>

<p>[9] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., … &amp; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).</p>

<p>[10] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning
some deep representations. CoRR, abs/1310.6343, 2013.</p>

<p>[11] Ioffe, S., &amp; Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.</p>

<p>[12] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).</p>]]></content><author><name></name></author><category term="studynote" /><category term="images," /><category term="links," /><category term="math" /><summary type="html"><![CDATA[Walkthrough on SOTA compact models.]]></summary></entry></feed>